import time
import threading
import httpx
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from sentence_transformers import SentenceTransformer, util

# ==============================
# üîπ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API
# ==============================
# –ü–æ–ª–æ–∂–∏ —Å–≤–æ–π –∫–ª—é—á –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª api_key.txt –∏ —Å—á–∏—Ç–∞–π –µ–≥–æ
with open("../api_key.txt", "r", encoding="utf-8") as f:
    API_KEY = f.read().strip()


# ==============================
# üîπ Rate Limit
# ==============================
MAX_REQUESTS_PER_MIN = 200
INTERVAL = 60 / MAX_REQUESTS_PER_MIN
_last_call_time = 0.0
_lock = threading.Lock()


def rate_limiter():
    """–ü—Ä–æ—Å—Ç–µ–π—à–∏–π –ª–æ–∫–∞–ª—å–Ω—ã–π rate limiter."""
    global _last_call_time
    with _lock:
        now = time.time()
        elapsed = now - _last_call_time
        if elapsed < INTERVAL:
            time.sleep(INTERVAL - elapsed)
        _last_call_time = time.time()


# ==============================
# üîπ –ó–∞–ø—Ä–æ—Å —Å retry –∏ timeout
# ==============================
@retry(
    retry=retry_if_exception_type((httpx.RequestError, httpx.TimeoutException)),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    stop=stop_after_attempt(5),
    reraise=True
)
def request_with_retry(url: str, params: dict = None, timeout: int = 10):
    rate_limiter()
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–ª—é—á –≤ ASCII –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
        api_key_ascii = str(API_KEY).encode("ascii", errors="ignore").decode("ascii")
        headers = {"Authorization": f"Bearer {api_key_ascii}"}

        with httpx.Client(timeout=timeout) as client:
            resp = client.get(url, params=params, headers=headers)
            if resp.status_code == 429:
                retry_after = int(resp.headers.get("Retry-After", "5"))
                print(f"‚ö†Ô∏è Rate limit, –∂–¥—É {retry_after} —Å–µ–∫...")
                time.sleep(retry_after)
                raise httpx.RequestError("Rate limited")
            resp.raise_for_status()
            return resp.json()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}, –ø–æ–≤—Ç–æ—Ä...")
        raise


# ==============================
# üîπ –†–∞–±–æ—Ç–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
# ==============================
model = SentenceTransformer("all-MiniLM-L6-v2")


def compare_texts(text1: str, text2: str) -> float:
    """–°—Ä–∞–≤–Ω–∏—Ç—å –¥–≤–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å–º—ã—Å–ª—É (–∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)."""
    emb1 = model.encode(text1, convert_to_tensor=True)
    emb2 = model.encode(text2, convert_to_tensor=True)
    similarity = util.cos_sim(emb1, emb2)
    return similarity.item()


def semantic_search(query: str, corpus: list, top_k: int = 3):
    """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
    corpus_embeddings = model.encode(corpus, convert_to_tensor=True)
    query_emb = model.encode(query, convert_to_tensor=True)

    similarities = util.cos_sim(query_emb, corpus_embeddings)[0]
    top_results = similarities.topk(k=top_k)

    results = []
    for score, idx in zip(top_results.values, top_results.indices):
        results.append((corpus[idx], float(score)))
    return results


# ==============================
# üîπ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
# ==============================
if __name__ == "__main__":
    print("=== –¢–µ—Å—Ç API-–∑–∞–ø—Ä–æ—Å–∞ —Å retry –∏ timeout ===")
    url = "https://httpbin.org/get"
    data = request_with_retry(url, params={"q": "test"})
    print(data)

    print("\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ ===")
    s = compare_texts("–Ø –ª—é–±–ª—é –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ.", "–ö–æ–¥–∏–Ω–≥ ‚Äì —ç—Ç–æ –º–æ—ë —Ö–æ–±–±–∏.")
    print(f"–°—Ö–æ–¥—Å—Ç–≤–æ: {s:.4f}")

    print("\n=== –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ ===")
    corpus = [
        "–ö–æ—à–∫–∏ –ª—é–±—è—Ç –º–æ–ª–æ–∫–æ.",
        "–°–æ–±–∞–∫–∏ –≤–µ—Ä–Ω—ã–µ –¥—Ä—É–∑—å—è —á–µ–ª–æ–≤–µ–∫–∞.",
        "–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Python –æ—á–µ–Ω—å –ø–æ–ø—É–ª—è—Ä–Ω–æ.",
        "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –º–µ–Ω—è–µ—Ç –º–∏—Ä.",
        "–ö–æ—Ç—è—Ç–∞ –º–∏–ª—ã–µ –∏ –∏–≥—Ä–∏–≤—ã–µ."
    ]
    query = "–ü–∏—Ç–∞–Ω–∏–µ –∫–æ—à–µ–∫"
    results = semantic_search(query, corpus, top_k=3)
    for text, score in results:
        print(f"{text} (—Å—Ö–æ–¥—Å—Ç–≤–æ {score:.4f})")
